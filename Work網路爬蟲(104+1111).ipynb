{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_Detail_104(detailurl):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    aa=[]\n",
    "#     link =[]\n",
    "#     title=[]\n",
    "#     corp=[]\n",
    "#     area=[]\n",
    "#     educational=[]\n",
    "#     experience=[]\n",
    "#     salary=[]\n",
    "#     dt=[]\n",
    "    from datetime import datetime\n",
    "    res = requests.get(detailurl)\n",
    "    soup = BeautifulSoup(res.text,\"lxml\")\n",
    "    for news in soup.select('.b-block__left'):\n",
    "        try:\n",
    "            if True:\n",
    "                link1 ='http:'+news.select_one('a.js-job-link').get('href')\n",
    "                title1 = news.select_one('a.js-job-link').text.strip()\n",
    "                corp1 = news.select_one('li a').text.strip()\n",
    "                area1 = news.select_one('ul.job-list-intro li').text.strip()\n",
    "                educational1=news.select_one('.job-list-intro').contents[9].text\n",
    "                experience1=news.select_one('.job-list-intro').contents[5].text\n",
    "                salary1=news.select_one('.b-tag--default').text\n",
    "                dt1=news.select_one('span.b-tit__date').text.strip()\n",
    "                aa.append({\n",
    "                    'title':title1,\n",
    "                    'dt':dt1,\n",
    "                    'link':link1,\n",
    "                    'area':area1,\n",
    "                    'corp':corp1,\n",
    "                    'salary':salary1,\n",
    "                    'experience':experience1,\n",
    "                    'educational':educational1,\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "    return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_104(a,n=3):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import time\n",
    "    newsary = []\n",
    "    for page in range(n):\n",
    "        url2 =('https://www.104.com.tw/jobs/search/?ro=0&keyword='+str(a)+'&area=6001001000%2C6001002000&order=12&asc=0&page={}&mode=s&jobsource=2018indexpoc'.format(page))\n",
    "        for dic in (page_Detail_104(url2)):\n",
    "            newsary.append(dic)\n",
    "    df = pd.DataFrame(newsary)\n",
    "    #對文字拆解\n",
    "    df[['city','area1']] = df['area'].str.extract('(.+)市(.+)區')\n",
    "    df['city']=df['city']+str('市')\n",
    "    df['area1'] = df['area1']+str('區')\n",
    "    df['experience']=df['experience'].str.extract('(\\d+?)年以上')\n",
    "    df['experience'].fillna(0,inplace=True)\n",
    "    df['experience'] = df['experience'].map(lambda e: int(e))\n",
    "    df[['sal_low','sal_high']]=df['salary'].map(lambda e: e.replace('月薪','').replace('元','').replace('待遇面議','0~0').replace(',','')).str.extract('(\\d+)~(\\d+)')\n",
    "    df.drop(['area','salary'],axis=1,inplace = True)\n",
    "    df['experience'].fillna('無經驗可',inplace=True)\n",
    "    df['remark'] = '104人力'\n",
    "    df2 = df[['title','corp','experience','educational','city','area1','dt','sal_low','sal_high','link','remark']]\n",
    "    df2=df2.sort_values(by=['experience'],ascending=[False])\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_Detail_1111(detailurl):    \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    res = requests.get(detailurl)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    aa =[]\n",
    "    bb =[]\n",
    "    for news in soup.select('div.jbInfoin'):\n",
    "        try:\n",
    "            if True:\n",
    "                title1 = news.select_one('a').get('title')\n",
    "                link1 = 'https:'+news.select_one('a').get('href')\n",
    "                corp1 = news.select_one('h4 a').text\n",
    "                area1 = news.select_one('span.location').text\n",
    "                educational1 = news.select_one('div.needs').contents[4]\n",
    "                experience1 = news.select_one('div.needs').contents[2]\n",
    "                salary1 = news.select_one('div.needs').contents[0].text\n",
    "                aa.append({  \n",
    "                    'title':title1,\n",
    "                    'link':link1,\n",
    "                    'area':area1,\n",
    "                    'corp':corp1,\n",
    "                    'salary':salary1,\n",
    "                    'experience':experience1,\n",
    "                    'educational':educational1,\n",
    "                            })\n",
    "        except:\n",
    "            pass\n",
    "    for dtime in soup.select('div.jbControl'):\n",
    "        dt1 = dtime.select_one('div.date').text\n",
    "        bb.append({'dt':dt1})\n",
    "        \n",
    "    return aa,bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_1111(a,n=3):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import time\n",
    "    newsary = []\n",
    "    for page in range(n):\n",
    "        url2 =('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ss=s&ks='+str(a)+'&c0=100100,100200&page={}'.format(page))\n",
    "        for dic in (page_Detail_1111(url2)):\n",
    "            newsary.append(dic)\n",
    "    df = pd.DataFrame(newsary[0])\n",
    "    df1= pd.DataFrame(newsary[1])\n",
    "    df = pd.concat([df,df1],axis = 1)\n",
    "    df[['city','area1']] = df['area'].str.extract('(.+)市(.+)區')\n",
    "    df['city']=df['city']+str('市')\n",
    "    df['area1'] = df['area1']+str('區')\n",
    "    df[['sal_low','sal_high']]=df['salary'].map(lambda e: e.replace('月薪','').replace('元','').replace('面議（經常性薪資4萬含以上）','0~0').replace(',','')).str.extract('(\\d+)~(\\d+)')\n",
    "    df['experience']=df['experience'].str.extract('(\\d+)')\n",
    "    df['experience'].fillna(0,inplace=True)\n",
    "    df['experience'] = df['experience'].map(lambda e: int(e))\n",
    "    df['educational']=df['educational'].str.extract('(..)')\n",
    "    df['educational'].replace('學歷','學歷不拘').replace('不拘','學歷不拘')\n",
    "    df['dt']=df['dt'].str.slice(5,7)+'/'+df['dt'].str.slice(8,10)\n",
    "    df['remark'] ='1111人力'\n",
    "#     print(type(df['dt']))\n",
    "    df3 = df[['title','corp','experience','educational','city','area1','dt','sal_low','sal_high','link','remark']]\n",
    "    df3=df3.sort_values(by=['experience'],ascending=[False])\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入職稱：大數據資料分析師\n",
      "請輸入頁數(1-30)：10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "SaveDirectory = os.getcwd()\n",
    "con = str(input('請輸入職稱：'))\n",
    "num = int(input('請輸入頁數(1-30)：'))\n",
    "SaveAs = os.path.join(SaveDirectory,'work_crawl'+'_'+str(con)+ time.strftime('%Y_%m_%d_%H_%M') + '.xlsx')\n",
    "if num <= 0:\n",
    "    print('數字不可小於1')\n",
    "elif num>30:\n",
    "    c104=crawling_104(a=con,n=30)\n",
    "    c1111=crawling_1111(a=con,n=30)\n",
    "else:\n",
    "    c104=crawling_104(a=con,n=num)\n",
    "    c1111=crawling_1111(a=con,n=num)\n",
    "    \n",
    "df = pd.concat([c104,c1111],axis=0)\n",
    "df.reset_index(inplace=True,drop=True) \n",
    "df[['mon','day']]=df['dt'].str.extract('(\\d+)/(\\d+)')\n",
    "df['mon']=df['mon'].fillna(0)\n",
    "df['day']=df['day'].fillna(0)\n",
    "df['mon']=df['mon'].map(lambda e: int(e))\n",
    "df['day']=df['day'].map(lambda e: int(e))\n",
    "#df[['title','corp','experience','educational','city','area1','dt','sal_low','sal_high','link','remark','mon','day']]\n",
    "df =df.sort_values(by=['mon','day','experience'],ascending=[True,True,False])\n",
    "df2=df[['title','corp','experience','educational','city','area1','dt','sal_low','sal_high','link','remark']]\n",
    "df2.to_excel(SaveAs,sheet_name='Work_Data',encoding='utf-8',startcol=1,startrow=1,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
